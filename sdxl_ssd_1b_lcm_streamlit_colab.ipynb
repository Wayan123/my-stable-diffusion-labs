{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade diffusers transformers accelerate peft streamlit"
      ],
      "metadata": {
        "id": "DBqT8IEuC3v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjVL-JqmBx3_"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from diffusers import DiffusionPipeline, AutoPipelineForText2Image\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Function to perform inference and display the result\n",
        "def perform_inference(prompt, num_inference_steps, guidance_scale, use_gpu, manual_seed):\n",
        "\n",
        "    # Initialize the Diffusion Pipeline\n",
        "    if use_gpu:\n",
        "        # Run on GPU\n",
        "        pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "        pipeline.to(\"cuda\")\n",
        "    else:\n",
        "        # Run on CPU\n",
        "        pipeline = DiffusionPipeline.from_pretrained(\"n1i/sdxl-ssd-1b-lcm\")\n",
        "\n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "\n",
        "    generator = torch.Generator(device=pipeline.device).manual_seed(manual_seed)\n",
        "    # Perform inference\n",
        "    result = pipeline(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale\n",
        "    ).images[0]\n",
        "\n",
        "    end_time = time.time()\n",
        "    latency_seconds = end_time - start_time\n",
        "    latency_minutes = int(latency_seconds // 60)\n",
        "    latency_seconds = int(latency_seconds % 60)\n",
        "\n",
        "    # Display the image\n",
        "    st.image(result, caption='Generated Image')\n",
        "\n",
        "    # Print the latency\n",
        "    st.write(\"Latency:\", latency_minutes, \"minutes and\", latency_seconds, \"seconds\")\n",
        "\n",
        "def main():\n",
        "    # Set page title\n",
        "    st.title(\"SDXL SSD 1B LCM\")\n",
        "\n",
        "    # Get prompt input from user\n",
        "    prompt = st.text_input(\"Enter your prompt:\", \"A cinematic shot of a baby raccoon wearing an intricate Italian priest robe, 8k.\")\n",
        "\n",
        "    # Create a sidebar for options\n",
        "    st.sidebar.title(\"Options\")\n",
        "    num_inference_steps = st.sidebar.slider(\"Number of Inference Steps\", min_value=1, max_value=10, value=1)\n",
        "    guidance_scale = st.sidebar.slider(\"Guidance Scale\", min_value=0.0, max_value=1.0, value=0.0)\n",
        "\n",
        "    # Add slider to set manual_seed value\n",
        "    manual_seed = st.sidebar.slider(\"Manual Seed\", min_value=0, max_value=9999, value=1337)\n",
        "\n",
        "    # Add radio button to choose GPU or CPU with CPU as default\n",
        "    use_gpu = st.sidebar.radio(\"Select device:\", (\"CPU\", \"GPU\"), index=0)\n",
        "\n",
        "    # Perform inference when the button is clicked\n",
        "    if st.button(\"Generate Image\"):\n",
        "        perform_inference(prompt, num_inference_steps, guidance_scale, use_gpu == \"GPU\", manual_seed)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "XJ-b90c4H4Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "LPlaHWsfH7x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "qXam65-PH-vH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}