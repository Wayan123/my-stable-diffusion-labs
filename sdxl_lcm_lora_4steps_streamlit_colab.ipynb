{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ref: https://huggingface.co/blog/lcm_lora"
      ],
      "metadata": {
        "id": "Zfl1o2tksIhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade diffusers transformers accelerate peft streamlit"
      ],
      "metadata": {
        "id": "DBqT8IEuC3v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from diffusers import DiffusionPipeline, LCMScheduler\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Function to perform inference and display the result\n",
        "def perform_inference(prompt, num_inference_steps, guidance_scale, use_gpu, manual_seed):\n",
        "\n",
        "    # Initialize the Diffusion Pipeline\n",
        "    if use_gpu:\n",
        "        # Run on GPU\n",
        "        model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "        lcm_lora_id = \"latent-consistency/lcm-lora-sdxl\"\n",
        "\n",
        "        pipe = DiffusionPipeline.from_pretrained(model_id, variant=\"fp16\")\n",
        "\n",
        "        pipe.load_lora_weights(lcm_lora_id)\n",
        "        pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "        pipe.to(device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "        st.write(\"We are currently painting your requested image, please wait and be patient as using GPU will only take a short while.\")\n",
        "    else:\n",
        "        # Run on CPU\n",
        "        model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "        lcm_lora_id = \"latent-consistency/lcm-lora-sdxl\"\n",
        "\n",
        "        pipe = DiffusionPipeline.from_pretrained(model_id, variant=\"fp16\")\n",
        "\n",
        "        pipe.load_lora_weights(lcm_lora_id)\n",
        "        pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "        st.write(\"We are currently painting your requested image, please wait and be patient as using CPU may take a bit longer.\")\n",
        "\n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "\n",
        "    generator = torch.Generator(device=pipe.device).manual_seed(manual_seed)\n",
        "    # Perform inference\n",
        "    result = pipe(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale\n",
        "    ).images[0]\n",
        "\n",
        "    end_time = time.time()\n",
        "    latency_seconds = end_time - start_time\n",
        "    latency_minutes = int(latency_seconds // 60)\n",
        "    latency_seconds = int(latency_seconds % 60)\n",
        "\n",
        "    # Display the image\n",
        "    st.image(result, caption='Generated Image')\n",
        "\n",
        "    # Print the latency\n",
        "    st.write(\"Latency:\", latency_minutes, \"minutes and\", latency_seconds, \"seconds\")\n",
        "\n",
        "    st.success(\"Congratulations, your picture has been successfully painted and we hope you are satisfied with the result.\")\n",
        "\n",
        "def main():\n",
        "    # Set page title\n",
        "    st.title(\"SDXL LCM LoRA (4 steps)\")\n",
        "\n",
        "    # Get prompt input from user\n",
        "    prompt = st.text_input(\"Enter your prompt:\", \"close-up photography of old man standing in the rain at night, in a street lit by lamps, leica 35mm summilux\")\n",
        "\n",
        "    # Create a sidebar for options\n",
        "    st.sidebar.title(\"Options\")\n",
        "    num_inference_steps = st.sidebar.slider(\"Number of Inference Steps\", min_value=1, max_value=10, value=4)\n",
        "    guidance_scale = st.sidebar.slider(\"Guidance Scale\", min_value=0.0, max_value=1.0, value=1.0)\n",
        "\n",
        "    # Add slider to set manual_seed value\n",
        "    manual_seed = st.sidebar.slider(\"Manual Seed\", min_value=0, max_value=9999, value=1337)\n",
        "\n",
        "    # Add radio button to choose GPU or CPU with CPU as default\n",
        "    use_gpu = st.sidebar.radio(\"Select device:\", (\"CPU\", \"GPU\"), index=0)\n",
        "\n",
        "    # Perform inference when the button is clicked\n",
        "    if st.button(\"Generate Image\"):\n",
        "        perform_inference(prompt, num_inference_steps, guidance_scale, use_gpu == \"GPU\", manual_seed)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AmlM6_GVNSOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "XJ-b90c4H4Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "LPlaHWsfH7x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "qXam65-PH-vH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}