{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Link referensi\n",
        "1. https://huggingface.co/n1i/sdxl-ssd-1b-lcm"
      ],
      "metadata": {
        "id": "y-P6BpXv-hTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade diffusers transformers accelerate peft"
      ],
      "metadata": {
        "id": "xJ0bWdUn-dvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.0. SDXL SSD 1B LCM - GPU"
      ],
      "metadata": {
        "id": "gePkNUGbZWug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from diffusers import DiffusionPipeline, AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "# pipeline = DiffusionPipeline.from_pretrained(\"n1i/sdxl-ssd-1b-lcm\")\n",
        "\n",
        "pipeline = AutoPipelineForText2Image.from_pretrained(\"n1i/sdxl-ssd-1b-lcm\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "pipeline.to(\"cuda\")\n",
        "\n",
        "prompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n",
        "\n",
        "image = pipeline(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=1,\n",
        "    guidance_scale=0.0).images[0]\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "T06pLPJRZUtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1. SDXL SSD 1B LCM - CPU"
      ],
      "metadata": {
        "id": "_WsHm3gyRWoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from diffusers import DiffusionPipeline, AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\"n1i/sdxl-ssd-1b-lcm\")\n",
        "\n",
        "# pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "# pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"A cinematic shot of a baby racoon wearing an intricate italian priest robe.\"\n",
        "\n",
        "image = pipeline(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=1,\n",
        "    guidance_scale=0.0).images[0]\n",
        "\n",
        "display(image)"
      ],
      "metadata": {
        "id": "K_z_y40AC0wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2. SDXL SSD 1B LCM - GPU"
      ],
      "metadata": {
        "id": "bnKOIjxGbucU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Initialize the Diffusion Pipeline\n",
        "pipeline = AutoPipelineForText2Image.from_pretrained(\"n1i/sdxl-ssd-1b-lcm\", torch_dtype=torch.float16, variant=\"fp16\")\n",
        "pipeline.to(\"cuda\")\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"A cinematic shot of a baby raccoon wearing an intricate Italian priest robe.\"\n",
        "\n",
        "# Perform inference for different values of num_inference_steps\n",
        "for num_steps in range(1, 5):\n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform inference\n",
        "    image = pipeline(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=num_steps,\n",
        "        guidance_scale=0.0\n",
        "    ).images[0]\n",
        "\n",
        "    end_time = time.time()\n",
        "    latency_seconds = end_time - start_time\n",
        "    latency_minutes = int(latency_seconds // 60)\n",
        "    latency_seconds = int(latency_seconds % 60)\n",
        "\n",
        "    # Display the image\n",
        "    display(image)\n",
        "\n",
        "    # Print the latency and num_inference_steps\n",
        "    print(f\"Num Inference Steps: {num_steps}, Latency: {latency_minutes} minutes and {latency_seconds} seconds\")"
      ],
      "metadata": {
        "id": "j5Kgyxddbrca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3. SDXL SSD 1B LCM - CPU"
      ],
      "metadata": {
        "id": "-zkUPWNqbUV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Initialize the Diffusion Pipeline\n",
        "pipeline = DiffusionPipeline.from_pretrained(\"n1i/sdxl-ssd-1b-lcm\")\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"A cinematic shot of a baby raccoon wearing an intricate Italian priest robe.\"\n",
        "\n",
        "# Perform inference for different values of num_inference_steps\n",
        "for num_steps in range(1, 5):\n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Perform inference\n",
        "    image = pipeline(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=num_steps,\n",
        "        guidance_scale=0.0\n",
        "    ).images[0]\n",
        "\n",
        "    end_time = time.time()\n",
        "    latency_seconds = end_time - start_time\n",
        "    latency_minutes = int(latency_seconds // 60)\n",
        "    latency_seconds = int(latency_seconds % 60)\n",
        "\n",
        "    # Display the image\n",
        "    display(image)\n",
        "\n",
        "    # Print the latency and num_inference_steps\n",
        "    print(f\"Num Inference Steps: {num_steps}, Latency: {latency_minutes} minutes and {latency_seconds} seconds\")\n"
      ],
      "metadata": {
        "id": "k_mDmB-vVBWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}